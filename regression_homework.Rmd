---
title: "Regression"
author: "Anna Yeaton"
date: "Fall 2019"
output:
  html_document:
    df_print: paged
fontsize: 11pt
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = T)
install.packages("caret")
install.packages("MASS")
install.packages("ridge")

```

# Lab Section

In this lab, we will go over regression. We will be using the caret package in R. https://topepo.github.io/caret/train-models-by-tag.html

# Perfomance Metrics 

## Residual 

Deviation of the observed value to the estimated value (sample mean)
$$residual=y_i - \hat{y_i}$$
where $\hat{y_i}$ is the estimated value

## Mean Squared Error (MSE)

$$MSE=\frac{1}{n}\sum_{i=1}^{n}(y_i - \hat{y_i})^2$$

## Root Mean Squared Error (RMSE)
Same units as original data.

$$RMSE=\sqrt{MSE}$$

## L2 regularization : Ridge regression. Regularize by adding the sum of the coefficients, squared, to the function. 

$$Ridge Regression=\sum_{i=1}^{n}(y_i - w_0 - \sum_{j=1}^{p}w_jx_{ij})^2 + \lambda\sum_{j=1}^p(w_j)^2$$

## L1 regularization : Lasso Regression. Regularize by adding the sum of the absolute value of the coefficients to the model. Coefficient estimates may be pushed to zero -- Lasso can perform variable selection

$$Lasso Regression=\sum_{i=1}^{n}(y_i - w_0 - \sum_{j=1}^{p}w_jx_{ij})^2 + \lambda\sum_{j=1}^p|w_j|$$


\newpage

### The broad steps of Machine learning in R. 

1. Split the data into training and test. Set test aside. 

2. Fit a good model to the training data. 

3. See how your model did on the training data.

4. Test how your model performs on the test data. 

# Regression

```{r, include=FALSE}
library(caret)
library(MASS)
library(ggplot2)
library(dplyr)
library(ggfortify)
library(ridge)

#Mauna Loa CO2 concentrations
data(airquality)
```


1. Split data into training and test set (75% in train set, 25% in test set)

```{r}

# view structure of data

str(airquality) # all numerical values, but missing values present
# investigate if missing values in relevant columns
sum(is.na(airquality$Temp))
sum(is.na(airquality$Wind))
# missing values not relevant as they are not in Temp or Wind

# set a random seed for reproducibility

set.seed(101)

# split data

training_size <- floor(0.75 * nrow(airquality))
training_index <- sample(seq_len(nrow(airquality)), size = training_size)

train_regression <- airquality[training_index, ]
test_regression <- airquality[-training_index, ]

nrow(train_regression) # 114 rows - confirms split is correct 
nrow(test_regression) # 39 rows

# view split data
train_regression
test_regression

```


### Linear Regression

* Assumes a linear relationship. 
* Independent variables should not be correlated (no mulitcollinearity)
* The number of observations should be greater than the number of independent variables.


$$RSS=\sum(y_i - \hat{y_i})^2$$
We will predict the response of the Temperature based on Wind. 

This is the data we will fit a linear model to. 
```{r}
# ggplot(data = train_regression) +
#   geom_point(aes(x=Wind, y=Temp)) +
#   theme_bw()
```

2. Create and fit a linear model to predict Temperature from Wind using the training set

```{r}
#help(train)

linear_regression <- train(Temp ~ Wind, data = train_regression, method = "lm")

```


3. Vizualize how your model performed on the train data by plotting the regression line on top of the train data points. 
```{r}

ggplot(data=train_regression, aes(x=Wind, y=Temp)) +
geom_point() +
geom_line(aes(y=fitted(linear_regression)), colour = 'red') +
theme_bw()

# plot shows reasonable distribution of residuals (both long and short) i.e. approximately equal variance around the regression line. Therefore, model has performed reasonably well, when assessed visually.

```


4. Explore how the model performs on the test data. For Linear Regression:

* The residuals should be close to zero.
* There should be equal variance around the regression line (homoscedasticity).
* Residuals should be normally distributed.
* Independent variables and residuals should not be correlated.

4 a) See how the model performs on the test data
```{r}
#help(predict)

linear_predict <- predict(linear_regression, newdata=test_regression)

```

4 b) Look at the residuals. Are they close to zero?
```{r}

#look at the median residual value. Close to zero is best
help(summary)

summary(linear_regression) # median residual value is 1.645

```


4 c) Plot predicted temperature vs observed temperature. A strong model should show a strong correlation
```{r}

# make a data frame first, for ease of comparison
results <- cbind(linear_predict,test_regression$Temp) 
colnames(results) <- c('predicted temperature','observed temperature')
results <- as.data.frame(results)
results

# plot
ggplot(data=results, aes(x=results$`observed temperature`, y=results$`predicted temperature`)) +
geom_point() +
labs(x = "observed temperature", y = "predicted temperature") +
theme_bw() 

# plot shows some degree of correlation between observed and predicted temperature values

```

4 d) Visualize the predicted values in relation to the real data points. Look for homoscedasticity
```{r}

# Extract coefficients from the model

intercept <- linear_regression$finalModel$coefficients[1] 
slope <- linear_regression$finalModel$coefficients[2]

intercept # 91.13478 
slope # -1.319254

# plot the regression line on the predicted values

ggplot(data=test_regression, aes(x=Wind, y=linear_predict)) +
geom_point() +
geom_abline(aes(intercept = intercept, slope = slope), colour = "red") +
labs(y = "Predicted Temperature") +
theme_bw()

# plot the original test values

# predicted and observed temperature values will be linked, for comparison

ggplot(data = test_regression) +
geom_point(aes(x=Wind, y=linear_predict, colour = 'predicted temperature')) +
geom_point(aes(x=Wind, y=Temp, colour = 'observed temperature')) +
geom_segment(aes(x = Wind, y = Temp, xend = Wind, yend = linear_predict)) +
labs(y = "Temperature") +
theme_bw()

```

4 e) Residuals should be normally distributed. Plot the density of the residuals
```{r}
#residuals_lin <- residuals(linear_regression)

#ggplot(data=residvpredict) +
#  geom_density(aes(residual))

ggplot() + geom_density(aes(residuals(linear_regression)))
# shows reasonable normal distribution


```


4 f) Independent variables and residuals should not be correlated
```{r}

cor.test(train_regression$Wind, resid(linear_regression))
# results, particularly p-value, do not show correlation

```


### Linear Regression with Regularization

5. Create a linear model using L1 or L2 regularization to predict Temperature from Wind and Month variables. Plot your predicted values and the real Y values on the same plot. 



```{r}

# check for missing values in Month
sum(is.na(airquality$Month)) # nil present

# ridge regression model
ridge_regression <- linearRidge(Temp ~ Wind + Month, data = train_regression, lambda = 'automatic')

# apply model to test data
ridge_predict <- predict(ridge_regression, newdata = test_regression)

# plot (x-axis = Wind)
ggplot(data = test_regression) +
geom_point(aes(x=Wind, ridge_predict, colour = 'predicted temperature')) +
geom_point(aes(x=Wind, y=Temp, colour = 'observed temperature')) +
geom_segment(aes(x = Wind, y = Temp, xend = Wind, yend = ridge_predict)) +
labs(y = 'Temperature') +
theme_bw()

# plot (x-axis = Month)
ggplot(data = test_regression) +
geom_point(aes(x=Month, y=ridge_predict, colour = 'predicted temperature')) +
geom_point(aes(x=Month, y=Temp, colour = 'observed temperature')) +
geom_segment(aes(x = Month, y = Temp, xend = Month, yend = ridge_predict)) +
labs(y = 'Temperature') +
theme_bw()

# make a data frame as well, for ease of comparison
results2 <- cbind(linear_predict, ridge_predict, test_regression$Temp) 
colnames(results2) <- c('predicted temperature', 'predictions with ridge regression','observed temperature')
results2 <- as.data.frame(results2)
results2

# SOME PREDICTIONS ARE CLOSER WITH RIDGE REGRESSION

```


